{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of 0 = 39\n",
      "Count of 1 = 37\n",
      "Count of 2 = 36\n",
      "Current Entropy is = 1.5841606263806214\n",
      "Splitting on feature X3 with gain ratio 0.8797391878691854\n",
      "Level 1\n",
      "Count of 0  =  39\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf node\n",
      "Level 1\n",
      "Count of 1 = 37\n",
      "Count of 2 = 36\n",
      "Current Entropy is = 0.9998646331239298\n",
      "Splitting on feature X3 with gain ratio 0.8238088893085824\n",
      "Level 2\n",
      "Count of 1 = 36\n",
      "Count of 2 = 1\n",
      "Current Entropy is = 0.1792560669283215\n",
      "Splitting on feature X2 with gain ratio 2.3304202679737513\n",
      "Level 3\n",
      "Count of 1  =  36\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf node\n",
      "Level 3\n",
      "Count of 2  =  1\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf node\n",
      "Level 2\n",
      "Count of 1 = 1\n",
      "Count of 2 = 35\n",
      "Current Entropy is = 0.18312206830137276\n",
      "Splitting on feature X2 with gain ratio 0.2753283606576246\n",
      "Level 3\n",
      "Count of 1 = 1\n",
      "Count of 2 = 1\n",
      "Current Entropy is = 1.0\n",
      "Splitting on feature X1 with gain ratio 1.0\n",
      "Level 4\n",
      "Count of 2  =  1\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf node\n",
      "Level 4\n",
      "Count of 1  =  1\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf node\n",
      "Level 3\n",
      "Count of 2  =  34\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf node\n",
      "----------------------------\n",
      "sklearn score= 0.9210526315789473\n",
      "our score= 0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Node:\n",
    "# for every node we ask a question,which have 2 answers true and false. split_col and split_value represents the question to ask\n",
    "#on which column and what value.(example- petal width<=0.8)and true false represents the direction to go if answer is true or false \n",
    "    def __init__(self,split_col,split_val,output):\n",
    "        self.split_col = split_col\n",
    "        self.split_val = split_val\n",
    "        self.output = output\n",
    "        self.true = None\n",
    "        self.false = None\n",
    "\n",
    "class ShoddyDTClassifier:\n",
    "    def __init__(self):\n",
    "        self.__root = None #root represents root of tree after fitting\n",
    "        \n",
    "    def __is_pure(self, y):\n",
    "    # checks if node is pure\n",
    "        unique_classes = np.unique(y)\n",
    "        if len(unique_classes) == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def __classify_data(self, y):\n",
    "    # in case node is pure it returns the only class in the node,otherwise returns class with majority\n",
    "        unique_classes,count = np.unique(y,return_counts=True) #returns 2 arrays, 1st with unique classes and 2nd with count of each classes\n",
    "        max_count_index = np.argmax(count) #argmax function gives index of largest number in array\n",
    "        classification = unique_classes[max_count_index]\n",
    "        return classification\n",
    "    \n",
    "    def __get_possible_splits(self,x):\n",
    "    # function returns dictionary of all the midpoints of continuous data with column index as keys and midpoints as values\n",
    "        possible_splits={}\n",
    "        for column_index in range(x.shape[1]):\n",
    "            possible_splits[column_index] = [] #creating list of values for each column index as keys\n",
    "            values = x[:,column_index]\n",
    "            unique_values = np.unique(values)\n",
    "            for i in range(len(unique_values)-1):\n",
    "                splits = (unique_values[i]+unique_values[i+1])/2 #getting midpoints of every 2 consecutive values\n",
    "                possible_splits[column_index].append(splits)\n",
    "        return possible_splits\n",
    "    \n",
    "    def __split_data(self,x,y,selected_feature,split_value):\n",
    "    #function returns 2 arrays one with target values below our split value one with above our split value\n",
    "    # example- data_below = all target values below 0.8, data_above = all target values above 0.8\n",
    "        data_below = y[x[:,selected_feature] <= split_value]\n",
    "        data_above = y[x[:,selected_feature] > split_value]\n",
    "        return data_below,data_above\n",
    "    \n",
    "    def __get_entropy(self,col):\n",
    "    #counts entropy\n",
    "        val_arr,count=np.unique(col,return_counts=True)\n",
    "        p=count/count.sum()\n",
    "        p_log=p*np.log2(p)\n",
    "        entropy= -p_log.sum()\n",
    "        return entropy\n",
    "    \n",
    "    def __get_overall_entropy(self,data_below,data_above):\n",
    "    # this function is used to determine the overall weighted entropy of data below and data above \n",
    "        total_data_points = len(data_below) + len(data_above)\n",
    "        p_data_below = len(data_below) / total_data_points\n",
    "        p_data_above = len(data_above) / total_data_points\n",
    "        overall_entropy = (p_data_below * self.__get_entropy(data_below)) + (p_data_above * self.__get_entropy(data_above))\n",
    "        return overall_entropy\n",
    "    \n",
    "    def __get_best_split(self,x,y,possible_splits):\n",
    "    # this function is used to get best split value(ex. 0.8) and best column index by calculating overall entropy\n",
    "        overall_entropy = np.inf\n",
    "        for col_index in possible_splits:\n",
    "            for values in possible_splits[col_index]:\n",
    "                data_below,data_above = self.__split_data(x,y,col_index,values)\n",
    "                curr_overall_entropy = self.__get_overall_entropy(data_below,data_above)\n",
    "                if curr_overall_entropy <= overall_entropy:\n",
    "                    overall_entropy = curr_overall_entropy\n",
    "                    best_split_column = col_index\n",
    "                    best_split_value = values\n",
    "        return best_split_column,best_split_value\n",
    "    \n",
    "    def __gain_ratio(self, selected_feature_index, split_value,x, y):\n",
    "    #calculates gain ratio for every split\n",
    "        original_entropy = self.__get_entropy(y)\n",
    "        initial_size = y.shape[0]\n",
    "        info_f = 0\n",
    "        split_info = 0\n",
    "        D1 = y[x[:,selected_feature_index] <= split_value]\n",
    "        D2 = y[x[:,selected_feature_index] > split_value]\n",
    "        info_f = (len(D1) / initial_size) * self.__get_entropy(D1) + (len(D2) / initial_size) * self.__get_entropy(D2)\n",
    "        split_info = -((len(D1) / initial_size) * np.log2(len(D1) / initial_size) + (len(D1) / initial_size) * np.log2(len(D1) / initial_size))\n",
    "        if split_info==0:\n",
    "            return np.inf\n",
    "        info_gain = original_entropy - info_f\n",
    "        gain_ratio=info_gain/split_info\n",
    "        return gain_ratio\n",
    "    \n",
    "    def __build_decision_tree(self,x,y,level,splits):\n",
    "        \n",
    "        # base case- if data is pure,classify the data and return a node with only output\n",
    "        if self.__is_pure(y):\n",
    "            classification = self.__classify_data(y)\n",
    "            print('Level',level)\n",
    "            for i in set(y):\n",
    "                print('Count of',i,' = ',len(y))\n",
    "                print('Current Entropy is = 0.0',)\n",
    "                print('Reached leaf node')\n",
    "            return Node(None,None,classification)\n",
    "        \n",
    "        #getting best feature to split upon\n",
    "        best_feature,best_split_value = self.__get_best_split(x,y,splits)\n",
    "        \n",
    "        # printing job\n",
    "        print('Level',level)\n",
    "        unique_classes,count = np.unique(y,return_counts=True)\n",
    "        for i in range(len(unique_classes)):\n",
    "            print('Count of',unique_classes[i],'=',count[i])\n",
    "        print('Current Entropy is =',self.__get_entropy(y))\n",
    "        print('Splitting on feature X'+str(best_feature),'with gain ratio',self.__gain_ratio(best_feature,best_split_value,x,y))\n",
    "        \n",
    "        #recursive part\n",
    "        classification = self.__classify_data(y) #classify data to put in node output\n",
    "        node = Node(best_feature,best_split_value,classification) #creating node\n",
    "        x_data_below = x[x[:,best_feature]<=best_split_value] # all the x data below split value(ex. 0.8)\n",
    "        x_data_above = x[x[:,best_feature]>best_split_value] # all the x data above split value\n",
    "        y_data_below = y[x[:,best_feature]<=best_split_value] #all the target data above split value\n",
    "        y_data_above = y[x[:,best_feature]>best_split_value] #all the target data above split value\n",
    "        true = self.__build_decision_tree(x_data_below,y_data_below,level+1,splits) #gets true answer\n",
    "        false = self.__build_decision_tree(x_data_above,y_data_above,level+1,splits) #gets false answer\n",
    "        node.true = true\n",
    "        node.false = false\n",
    "        return node #returning root of tree\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        splits = self.__get_possible_splits(x)\n",
    "        self.__root = self.__build_decision_tree(x,y,0,splits)\n",
    "        \n",
    "    def __predict_helper(self,x,root):\n",
    "    #starts from root node and asks a question to find out which direction to go    \n",
    "        \n",
    "        #base case- if root has no child,return the output\n",
    "        if root.true == None and root.false == None:\n",
    "            return root.output\n",
    "        \n",
    "        #recursive part\n",
    "        if x[root.split_col] <= root.split_val: # if data is less than specified value we go towards true\n",
    "            return self.__predict_helper(x,root.true)\n",
    "        else:\n",
    "            return self.__predict_helper(x,root.false) #otherwise we go towards false\n",
    "        \n",
    "    def predict(self,x):\n",
    "        y = np.array([0 for i in range(len(x))])\n",
    "        for row in range(len(x)):\n",
    "            y[row] = self.__predict_helper(x[row],self.__root)\n",
    "        return y\n",
    "    \n",
    "    def score(self,x,y):\n",
    "    #returns mean accuracy\n",
    "        y_pred = self.predict(x)\n",
    "        count = 0\n",
    "        for i in range(len(y)):\n",
    "            if y_pred[i] == y[i]:\n",
    "                count+=1\n",
    "        return count/len(y)\n",
    "\n",
    "#data preparation\n",
    "from sklearn.datasets import load_iris\n",
    "iris=load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y) \n",
    "clf=DecisionTreeClassifier()\n",
    "s_clf=ShoddyDTClassifier()\n",
    "clf.fit(x_train,y_train)\n",
    "s_clf.fit(x_train,y_train)\n",
    "c=clf.score(x_test,y_test)\n",
    "s=s_clf.score(x_test,y_test)\n",
    "print('----------------------------')\n",
    "print('sklearn score=',c)\n",
    "print('our score=',s)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
